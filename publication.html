<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Publications - Sangwon Yoon</title>
  <link rel="stylesheet" href="style.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
</head>
<body>

  <nav class="top-nav">
    <div class="nav-container">
      <div class="nav-name">SANGWON YOON</div>
      <button class="menu-toggle" onclick="document.querySelector('.nav-links').classList.toggle('show')">&#9776;</button>
      <ul class="nav-links">
        <li><a href="index.html">Home</a></li>
        <li><a href="publication.html" class="active">Publication</a></li>
        <li><a href="portfolio.html">Angel Portfolio</a></li>
      </ul>
    </div>
  </nav>

  <div class="container">
    <div class="page-header">
      <h1>Publications</h1>
    </div>

    <!-- Unlearning-Aware Minimization -->
    <div class="pub-card-inline">
      <div class="pub-card-header">
        <span class="pub-venue-badge">NeurIPS 2025</span>
        <span class="pub-card-title"><a href="https://openreview.net/forum?id=kAuckbcMvi" target="_blank">Unlearning-Aware Minimization</a></span>
      </div>
      <div class="pub-authors">
        Hoki Kim, Keonwoo Kim, Sungwon Chae, <span class="me">Sangwon Yoon</span>
      </div>
      <p class="pub-abstract">Machine unlearning aims to remove the influence of specific training samples (i.e., forget data) from a trained model while preserving its performance on the remaining samples (i.e., retain data). Existing approximate unlearning approaches, such as fine-tuning or negative gradient, often suffer from either insufficient forgetting or significant degradation on retain data. In this paper, we introduce Unlearning-Aware Minimization (UAM), a novel min–max optimization framework for machine unlearning. UAM perturbs model parameters to maximize the forget loss and then leverages the corresponding gradients to minimize the retain loss. We derive an efficient optimization method for this min-max problem, which enables effective removal of forget data and uncovers better optima that conventional methods fail to reach. Extensive experiments demonstrate that UAM outperforms existing methods across diverse benchmarks, including image classification datasets (CIFAR-10, CIFAR-100, TinyImageNet) and multiple-choice question-answering benchmarks for large language models (WMDP-Bio, WMDP-Cyber).</p>
    </div>

    <!-- Vocal Delivery Quality -->
    <div class="pub-card-inline">
      <div class="pub-card-header">
        <span class="pub-venue-badge">JAE 2025</span>
        <span class="pub-card-title"><a href="https://www.sciencedirect.com/science/article/pii/S0165410124000934" target="_blank">Vocal Delivery Quality in Earnings Conference Calls</a></span>
      </div>
      <div class="pub-authors">
        Bok Baik*, Alex Kim*, David Sunghyo Kim*, <span class="me">Sangwon Yoon</span>*
        <span class="equal-contrib">(* equal contribution)</span>
      </div>
      <p class="pub-abstract">We study the economic consequences of managers' vocal delivery quality during earnings conference calls. We introduce a novel measure, vocal delivery quality, that captures the acoustic comprehensibility of audio information for an average listener. Our measure relies on a deep-learning algorithm applied to a large sample of earnings call audio files. Consistent with predictions from the psychology and accounting literatures, we find evidence that the quality of managers' vocal delivery deteriorates when they deliver negative news, such as a decrease in earnings or negative narrative information, and positive but transitory earnings news. We show that the stock market reacts in real time to managers' vocal delivery quality. We also document that the vocal delivery quality has an effect on information intermediaries such as analysts and the media. Overall, our findings underscore the role of vocal dimensions in financial communication.</p>
    </div>

    <!-- Bridging Language Models -->
    <div class="pub-card-inline">
      <div class="pub-card-header">
        <span class="pub-venue-badge">arXiv 2025</span>
        <span class="pub-card-title"><a href="https://arxiv.org/abs/2503.22693" target="_blank">Bridging Language Models and Financial Analysis</a></span>
      </div>
      <div class="pub-authors">
        Alejandro Lopez-Lira*, Jihoon Kwon*, <span class="me">Sangwon Yoon</span>*, Jy-yong Sohn*, Chanyeol Choi*
        <span class="equal-contrib">(* equal contribution)</span>
      </div>
      <p class="pub-abstract">The rapid advancements in Large Language Models (LLMs) have unlocked transformative possibilities in natural language processing, particularly within the financial sector. Financial data is often embedded in intricate relationships across textual content, numerical tables, and visual charts, posing challenges that traditional methods struggle to address effectively. However, the emergence of LLMs offers new pathways for processing and analyzing this multifaceted data with increased efficiency and insight. Despite the fast pace of innovation in LLM research, there remains a significant gap in their practical adoption within the finance industry, where cautious integration and long-term validation are prioritized. This disparity has led to a slower implementation of emerging LLM techniques, despite their immense potential in financial applications. As a result, many of the latest advancements in LLM technology remain underexplored or not fully utilized in this domain. This survey seeks to bridge this gap by providing a comprehensive overview of recent developments in LLM research and examining their applicability to the financial sector. Building on previous survey literature, we highlight several novel LLM methodologies, exploring their distinctive capabilities and their potential relevance to financial data analysis. By synthesizing insights from a broad range of studies, this paper aims to serve as a valuable resource for researchers and practitioners, offering direction on promising research avenues and outlining future opportunities for advancing LLM applications in finance.</p>
    </div>

    <!-- DEBATE -->
    <div class="pub-card-inline">
      <div class="pub-card-header">
        <span class="pub-venue-badge">ACL 2024</span>
        <span class="pub-card-title"><a href="https://aclanthology.org/2024.findings-acl.112/" target="_blank">DEBATE: Devil's Advocate-Based Assessment and Text Evaluation</a></span>
      </div>
      <div class="pub-authors">
        Alex Kim*, Gunwoo Kim*, <span class="me">Sangwon Yoon</span>*
        <span class="equal-contrib">(* equal contribution)</span>
      </div>
      <p class="pub-abstract">As natural language generation (NLG) models have become prevalent, systematically assessing the quality of machine-generated texts has become increasingly important. Recent studies introduce LLM-based evaluators that operate as reference-free metrics, demonstrating their capability to adeptly handle novel tasks. However, these models generally rely on a single-agent approach, which, we argue, introduces an inherent limit to their performance. This is because there exist biases in LLM agent's responses, including preferences for certain text structure or content. In this work, we propose DEBATE, an NLG evaluation framework based on multi-agent scoring system augmented with a concept of Devil's Advocate. Within the framework, one agent is instructed to criticize other agents' arguments, potentially resolving the bias in LLM agent's answers. DEBATE substantially outperforms the previous state-of-the-art methods in two meta-evaluation benchmarks in NLG evaluation, SummEval and TopicalChat. We also show that the extensiveness of debates among agents and the persona of an agent can influence the performance of evaluators.</p>
    </div>

    <!-- Detecting Rumor Veracity -->
    <div class="pub-card-inline">
      <div class="pub-card-header">
        <span class="pub-venue-badge">SocialNLP 2022</span>
        <span class="pub-card-title"><a href="https://aclanthology.org/2022.socialnlp-1.3/" target="_blank">Detecting Rumor Veracity with Only Textual Information by Double-Channel Structure</a></span>
      </div>
      <div class="pub-authors">
        Alex Gunwoo Kim*, <span class="me">Sangwon Yoon</span>*
        <span class="equal-contrib">(* equal contribution)</span>
      </div>
      <p class="pub-abstract">Kyle (1985) proposes two types of rumors: informed rumors which are based on some private information and uninformed rumors which are not based on any information (ie bluffing). Also, prior studies find that when people have credible source of information, they are likely to use a more confident textual tone in their spreading of rumors. Motivated by these theoretical findings, we propose a double-channel structure to determine the ex-ante veracity of rumors on social media. Our ultimate goal is to classify each rumor into true, false, or unverifiable category. We first assign each text into either certain (informed rumor) or uncertain (uninformed rumor) category. Then, we apply lie detection algorithm to informed rumors and thread-reply agreement detection algorithm to uninformed rumors. Using the dataset of SemEval 2019 Task 7, which requires ex-ante threefold classification (true, false, or unverifiable) of social media rumors, our model yields a macro-F1 score of 0.4027, outperforming all the baseline models and the second-place winner (Gorrell et al., 2019). Furthermore, we empirically validate that the double-channel structure outperforms single-channel structures which use either lie detection or agreement detection algorithm to all posts.</p>
    </div>

    <!-- BlahBlahBot -->
    <div class="pub-card-inline">
      <div class="pub-card-header">
        <span class="pub-venue-badge">CHI 2021</span>
        <span class="pub-card-title"><a href="https://dl.acm.org/doi/abs/10.1145/3411763.3451771" target="_blank">BlahBlahBot: Facilitating Conversation Between Strangers Using a Chatbot with ML-Infused Personalized Topic Suggestion</a></span>
      </div>
      <div class="pub-authors">
        Donghoon Shin, <span class="me">Sangwon Yoon</span>, Soomin Kim, Joonhwan Lee
      </div>
      <p class="pub-abstract">It is a prevalent behavior of having a chat with strangers in online settings where people can easily gather. Yet, people often find it difficult to initiate and maintain conversation due to the lack of information about strangers. Hence, we aimed to facilitate conversation between the strangers with the use of machine learning (ML) algorithms and present BlahBlahBot, an ML-infused chatbot that moderates conversation between strangers with personalized topics. Based on social media posts, BlahBlahBot supports the conversation by suggesting topics that are likely to be of mutual interest between users. A user study with three groups (control, random topic chatbot, and BlahBlahBot; N=18) found the feasibility of BlahBlahBot in increasing both conversation quality and closeness to the partner, along with the factors that led to such increases from the user interview.</p>
    </div>

    <!-- Corporate Bankruptcy Prediction -->
    <div class="pub-card-inline">
      <div class="pub-card-header">
        <span class="pub-venue-badge">EconLP 2021</span>
        <span class="pub-card-title"><a href="https://aclanthology.org/2021.econlp-1.4/" target="_blank">Corporate Bankruptcy Prediction with Domain-Adapted BERT</a></span>
      </div>
      <div class="pub-authors">
        Alex Gunwoo Kim*, <span class="me">Sangwon Yoon</span>*
        <span class="equal-contrib">(* equal contribution)</span>
      </div>
      <p class="pub-abstract">This study performs BERT-based analysis, which is a representative contextualized language model, on corporate disclosure data to predict impending bankruptcies. Prior literature on bankruptcy prediction mainly focuses on developing more sophisticated prediction methodologies with financial variables. However, in our study, we focus on improving the quality of input dataset. Specifically, we employ BERT model to perform sentiment analysis on MD&A disclosures. We show that BERT outperforms dictionary-based predictions and Word2Vec-based predictions in terms of adjusted R-square in logistic regression, k-nearest neighbor (kNN-5), and linear kernel support vector machine (SVM). Further, instead of pre-training the BERT model from scratch, we apply self-learning with confidence-based filtering to corporate disclosure data (10-K). We achieve the accuracy rate of 91.56% and demonstrate that the domain adaptation procedure brings a significant improvement in prediction accuracy.</p>
    </div>

    <!-- Self-Adapter at SemEval -->
    <div class="pub-card-inline">
      <div class="pub-card-header">
        <span class="pub-venue-badge">SemEval 2021</span>
        <span class="pub-card-title"><a href="https://aclanthology.org/2021.semeval-1.55/" target="_blank">Self-Adapter at SemEval-2021 Task 10: Entropy-Based Pseudo-Labeler for Source-Free Domain Adaptation</a></span>
      </div>
      <div class="pub-authors">
        <span class="me">Sangwon Yoon</span>, Yanghoon Kim, Kyomin Jung
      </div>
      <p class="pub-abstract">Source-free domain adaptation is an emerging line of work in deep learning research since it is closely related to the real-world environment. We study the domain adaption in the sequence labeling problem where the model trained on the source domain data is given. We propose two methods: Self-Adapter and Selective Classifier Training. Self-Adapter is a training method that uses sentence-level pseudo-labels filtered by the self-entropy threshold to provide supervision to the whole model. Selective Classifier Training uses token-level pseudo-labels and supervises only the classification layer of the model. The proposed methods are evaluated on data provided by SemEval-2021 task 10 and Self-Adapter achieves 2nd rank performance.</p>
    </div>

    <!-- 민사법학 -->
    <div class="pub-card-inline">
      <div class="pub-card-header">
        <span class="pub-venue-badge">민사법학 2021</span>
        <span class="pub-card-title"><a href="https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002703205" target="_blank">Illegality and Extinctive Prescription in State Compensation on Yushin Emergency Measures</a></span>
      </div>
      <div class="pub-authors">
        <span class="me">Sangwon Yoon</span>, Hyeyeon Yoon
      </div>
      <p class="pub-abstract">This paper organizes the legal issues of the State Compensation claim on emergency measures. This is divided into illegality and extinctive prescription. In terms of illegality, pros and cons were raised over the Supreme Court's attitude that the actions of the president who issued emergency measures, the investigative agencies and judges who investigated and ruled based on emergency measures, were not illegal acts under Article 2 paragraph 1 of the National Compensation Act. Illegality should be discussed with regard to an act of the state, not as an individual institution. However, even if an individual state agency's actions are reviewed for illegality, it should be recognized for their illegality, unlike the Supreme Court's ruling. As for the extinctive prescription, the Supreme Court had previously acknowledged the completion of the extinctive prescription and ruled it on the basis of the theory of abuse of rights. But in 2018, the Constitutional Court ruled that applying the Article 766 paragraph 2 of the Civil Act on State Crime case is unconstitutional, excluding the application of objective starting points of extinctive prescription on State Crime. The Binding force of the 2014Hunma148 was confirmed by the Supreme Court, and thus only subjective starting point is applied to the extinctive prescription for state compensation for emergency measures.</p>
    </div>

    <!-- 비교경제연구 -->
    <div class="pub-card-inline">
      <div class="pub-card-header">
        <span class="pub-venue-badge">비교경제연구 2019</span>
        <span class="pub-card-title"><a href="https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002535729" target="_blank">The Political Economic Logic of DPRK Nuclear Strategy: Separative Engagement as a Proxy Solution</a></span>
      </div>
      <div class="pub-authors">
        <span class="me">Sangwon Yoon</span>*, Yongseok Kwon*
        <span class="equal-contrib">(* equal contribution)</span>
      </div>
      <p class="pub-abstract">This paper analyzes the relationship between the political economic structure and foreign policy of North Korea. By mathematically modelling domestic interactions such as market control, escaping, and corruption of sheriffs, we argue that the fundamental agency problem of personalistic dictatorship can be adduced as the main reason of nuclear development. The traditional foreign strategies of international community, both economic sanction which demands preemptive denuclearization and structural engagement which induces transition to capitalism, can fail because of filtering problem. The separative engagement which we adduce as an alternative strategy intends to take the strengths and make up for the weakness of the traditional strategies. The deviation of North Korea can be effectively prevented by forming an international economic belt, and the possibility of making a big deal increases as the tranformative effect of engagement vanishes. The denuclearization of North Korea can be achieved only by analyzing the incentive structure of regime in depth, not by a haphazard jumble of improvisation.</p>
    </div>

  </div>

  <footer>
    <p>&copy; 2026 Sangwon Yoon. All Rights Reserved.</p>
  </footer>

</body>
</html>
